---
title: "STOR767 - Computational Problems for HW 2"
author: Brian N. White
output:
  pdf_document: default
  html_document: default
  word_document: default
---


## Problem: Logistic Regression and Classification

```{r load packages}
library(tidyverse)
library(leaps)
library(bestglm)
library(pROC)
```


```{r data preparation, include=F}
# Notice that we hide the code and the results here
# Using `include=F` in the chunk declaration. 

hd_data <-  read.csv("FRAMINGHAM.dat.txt")
### Renames, setting the variables with correct natures...
names(hd_data)[1] <- "HD"
hd_data$HD <- as.factor(hd_data$HD)
hd_data$SEX <- as.factor(hd_data$SEX)
str(hd_data)
#tail(hd_data, 1)    # The last row is for prediction
hd_data.new <- hd_data[1407,] # The female whose HD will be predicted.
hd_data <- hd_data[-1407,]  # take out the last row 
hd_data.f <- na.omit(hd_data)
```

We note that this dataset contains 307 people diagnosed with heart disease and 1086 without heart disease.
```{r table heart disease, echo = F, comment = " "}
# we use echo = F to avoid showing this R code
table(hd_data.f$HD) # HD: 307 of "0" and 1086 "1" 
```

After a quick cleaning up here is a summary about the data:
```{r data summary}
summary(hd_data.f)
```

**A)**
Create a training dataset with 1000 observations and a testing dataset with the rest of the data. Using `set.seed(1)`.

**Solution**

```{r create training and testing data sets}
set.seed(1)

#number of total observations
N <- length(hd_data.f$HD)
#generate indices for training data
index.train <- sample(N, 1000)
#training data
hd_data.train <- hd_data.f[index.train,]
#testing data
hd_data.test <- hd_data.f[-index.train,]
```


**B)** 
Our goal is to fit a well-fitting model, that is still small and easy to interpret (parsimonious).

1. Use AIC as the criterion for model selection. Find a logistic regression model with small AIC through exhaustive search in the training dataset. Call this model `fit.aic`.

**Solution**

An exhaustive search using AIC as the selection criterion returned the logistic regression model with six predictors: AGE, SEX, SBP, CHOL, FRW and CIG. The optimal model, according to this search, has an AIC value of 941.42. Note, I used the package, and corresponding command, 'bestglm' to perform this search.

```{r model selection for glm using exhaustive search with AIC selection criterion}
#create data frame of the form Xy where X is the design matrix and y is the response vector
df_glm <- data.frame(cbind(hd_data.train[,-1], HD=hd_data.train[,1]))
#use the bestglm package to perform model selection via exhaustive search using AIC as the selection criterion
exhaustive_search <- bestglm(df_glm, family = binomial, IC="AIC", method = "exhaustive")
#returns the best n predictor models where n ranges from 0 to 7
exhaustive_search$Subsets
#the best model over the exhaustive search is the model with the 6 predictors specified via the output below
exhaustive_search$BestModel
#fit the selected model
fit.aic <- glm(HD ~ AGE + SEX + SBP + CHOL + FRW + CIG, family=binomial, data=hd_data.train)
#AIC is 941.4
summary(fit.aic)
```

2. Use the model chosen from part B1 as the final model. Write a brief summary to describe important factors relating to Heart Diseases (i.e. the relationships between those variables in the model and heart disease). Make sure to define important factors in your words.

**Solution**

All of the  Wald tests, except for FRW, are statistically significant at the $\alpha=0.05$ level. Thus, the data suggest that there is, in fact, a linear association between the co-variates and the logit of heart disease. To be more precise, each of the variables are positively correlated with the logit of heart disease. Of the co-variates, the sex and, to a much lesser extent, the age of an individual have the greatest impact on the logit of heart disease (e.g. An increase of one year results in a mean increase of 0.94 for the logit of heart disease).

3. Liz is a patient with the following readings: `AGE=50, GENDER=FEMALE, SBP=110, DBP=80, CHOL=180, FRW=105, CIG=0`. What is the probability that she will have heart disease, according to our final model?

**Solution**

The probability that Liz will have heart disease, according to the final model, is ~0.04.

```{r return predicted probabilty of heart disease for new observation}
#create data.frame to store new observation
new_obs <- data.frame(AGE=50, SEX="FEMALE", SBP=110, DBP=80, CHOL=180, FRW=105, CIG=0)
#returns predicted probability of heart disease for new observation
predict(fit.aic, new_obs, type="response")
```

4. Consider using `fit.aic` for classification in the test dataset. Display the ROC curve using `fit.aic`. Explain what ROC reports and how to use the graph. 

**Solution**

```{r display and discuss the ROC curve for fit.aic}
fit.aic.test <- predict(fit.aic, hd_data.test, type="response")
fit.aic.roc <- roc(hd_data.test$HD, fit.aic.test, plot=T)
```


**C)** 
1. Use BIC as the criterion for model selection. Find a logistic regression model with small BIC through exhaustive search. Call this model `fit.bic`. Compare `fit.bic` and `fit.aic`.

**Solution**

An exhaustive search using BIC as the selection criterion returned the logistic regression model with four predictors: AGE, SEX, SBP and CHOL. The optimal model, according to this search, has an BIC value of 961.86. (note: The code-chunk below is a modified version of the code-chunk in part B1. Only the information criterion argument, IC, in the bestglm has been changed)

```{r model selection for glm using exhaustive search with BIC selection criterion}
#use the bestglm package to perform model selection via exhaustive search using BIC as the selection criterion
exhaustive_search2 <- bestglm(df_glm, family = binomial, IC="BIC", method = "exhaustive")
#returns the best n predictor models where n ranges from 0 to 7
exhaustive_search2$Subsets
#the best model over the exhaustive search is the model with the 4 predictors specified via the output below
exhaustive_search2$BestModel
#fit the selected model
fit.bic <- glm(HD ~ AGE + SEX + SBP + CHOL, family=binomial, data=hd_data.train)
#BIC is 961.8568
summary(fit.bic)
```


Now, let's compare the two models. 

```{r comparision of fit.bic to fit.aic}
fit.aic
fit.bic
```

2. Overlay two ROC curves with the test dataset: One from `fit.bic`, the other from `fit.aic` from part A1. Based on the ROC curves, which one do you prefer? 

**Solution**

Based upon the plot below I would prefer to use fit.bic, as it has the greater of the two AUC values (i.e. AUC(fit.bic)=0.69 > AUC(fit.aic)=0.68).

```{r compare ROC curves associated with fit.bic and fit.aic, include=FALSE }
par(mfrow=c(1, 2))
fit.bic.test <- predict(fit.bic, hd_data.test, type="response")

fit.bic.roc <- roc(hd_data.test$HD, fit.bic.test, plot=T)
fit.aic.roc <- roc(hd_data.test$HD, fit.aic.test, plot=T)
```

```{r generates overlayed ROC curves mentioned in question}
plot(1-fit.aic.roc$specificities, fit.aic.roc$sensitivities, col="red", pch=16,type="l",
     xlab=paste("AUC(fit.aic) =",
                round(pROC::auc(fit.aic.roc),2),
                "  AUC(fit.bic) =",
                round(pROC::auc(fit.bic.roc),2) ), 
     ylab="Sensitivities")  

points(1-fit.bic.roc$specificities, fit.bic.roc$sensitivities, col="blue",type="l", pch=16)
legend("bottomright", legend=c("fit.aic w/ six variables", "fit.bic w/ four variable"),
       lty=c(1,1), lwd=c(2,2), col=c("red", "blue"))
title("Comparison of two models using testing data")
```

