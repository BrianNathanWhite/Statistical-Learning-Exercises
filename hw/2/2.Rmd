---
title: "STOR767 - Computational Problems for HW 2"
author: Brian N. White
output:
  html_document: default
  pdf_document: default
  word_document: default
---


## Problem: Logistic Regression and Classification

```{r load packages}
library(tidyverse)
library(leaps)
library(bestglm)
```


```{r data preparation, include=F}
# Notice that we hide the code and the results here
# Using `include=F` in the chunk declaration. 

hd_data <-  read.csv("FRAMINGHAM.dat.txt")
### Renames, setting the variables with correct natures...
names(hd_data)[1] <- "HD"
hd_data$HD <- as.factor(hd_data$HD)
hd_data$SEX <- as.factor(hd_data$SEX)
str(hd_data)
#tail(hd_data, 1)    # The last row is for prediction
hd_data.new <- hd_data[1407,] # The female whose HD will be predicted.
hd_data <- hd_data[-1407,]  # take out the last row 
hd_data.f <- na.omit(hd_data)
```

We note that this dataset contains 307 people diagnosed with heart disease and 1086 without heart disease.
```{r table heart disease, echo = F, comment = " "}
# we use echo = F to avoid showing this R code
table(hd_data.f$HD) # HD: 307 of "0" and 1086 "1" 
```

After a quick cleaning up here is a summary about the data:
```{r data summary}
summary(hd_data.f)
```

**A)**
Create a training dataset with 1000 observations and a testing dataset with the rest of the data. Using `set.seed(1)`.

**Solution**

```{r create training and testing data sets}
set.seed(1)

#number of total observations
N <- length(hd_data.f$HD)
#generate indices for training data
index.train <- sample(N, 1000)
#training data
hd_data.train <- hd_data.f[index.train,]
#testing data
hd_data.test <- hd_data.f[-index.train,]
```


**B)** 
Our goal is to fit a well-fitting model, that is still small and easy to interpret (parsimonious).

1. Use AIC as the criterion for model selection. Find a logistic regression model with small AIC through exhaustive search in the training dataset. Call this model `fit.aic`.

**Solution**

An exhaustive search using AIC as the selection criterion returned the logistic regression model with predictors AGE, SEX, SBP, CHOL, FRW and CIG. The optimal model, according to this search, has an AIC value of 941.42. Note, I used the package, and corresponding command, 'bestglm' to perform this search.

```{r model selection for glm using exhaustive search with AIC selection criterion}
#create data frame of the form Xy where X is the design matrix and y is the response vector
df_glm <- data.frame(cbind(hd_data.train[,-1], HD=hd_data.train[,1]))
#use the bestglm package to perform model selection via exhaustive search using AIC as the selection criterion
exhaustive_search <- bestglm(df_glm, family = binomial, IC="AIC", method = "exhaustive")
#returns the best n predictor models where n ranges from 0 to 7
exhaustive_search$Subsets
#the best model over the exhaustive search is the model with the 6 predictors specified via the output below
exhaustive_search$BestModel
#fit the selected model
fit.aic <- glm(HD ~ AGE + SEX + SBP + CHOL + FRW + CIG, family=binomial, data=hd_data.train)
#AIC is 941.4
summary(fit.aic)
```

2. Use the model chosen from part B1 as the final model. Write a brief summary to describe important factors relating to Heart Diseases (i.e. the relationships between those variables in the model and heart disease). Make sure to define Ã¢importatn factors in your words.

3. Liz is a patient with the following readings: `AGE=50, GENDER=FEMALE, SBP=110, DBP=80, CHOL=180, FRW=105, CIG=0`. What is the probability that she will have heart disease, according to our final model?

```{r}

```


4. Consider using `fit.aic` for classification in the test dataset. Display the ROC curve using `fit.aic`. Explain what ROC reports and how to use the graph. 

**C)** 
1. Use BIC as the criterion for model selection. Find a logistic regression model with small BIC through exhaustive search. Call this model `fit.bic`. Compare `fit.bic` and `fit.aic`.

**Solution**

An exhaustive search using BIC as the selection criterion returned the logistic regression model with predictors AGE, SEX, SBP and CHOL. The optimal model, according to this search, has an BIC value of 961.86. (note: The code-chunk below is a modified version of the code-chunk in part B1. Only the information criterion argument, IC, in the bestglm has been changed)

```{r model selection for glm using exhaustive search with BIC selection criterion}
#use the bestglm package to perform model selection via exhaustive search using BIC as the selection criterion
exhaustive_search2 <- bestglm(df_glm, family = binomial, IC="BIC", method = "exhaustive")
#returns the best n predictor models where n ranges from 0 to 7
exhaustive_search2$Subsets
#the best model over the exhaustive search is the model with the 4 predictors specified via the output below
exhaustive_search2$BestModel
#fit the selected model
fit.bic <- glm(HD ~ AGE + SEX + SBP + CHOL, family=binomial, data=hd_data.train)
#BIC is 961.8568
summary(fit.bic)
```

```{r comparision of fit.bic to fit.aic}
fit.aic
fit.bic
```

2. Overlay two ROC curves with the test dataset: One from `fit.bic`, the other from `fit.aic` from part A1. Based on the ROC curves, which one do you prefer? 

```{r compare ROC curves associated with fit.bic and fit.aic}

```

